# ğŸ™ï¸ Chicago 311 Service Request Intelligence Platform

> **A Production-Grade ML Portfolio Project** demonstrating end-to-end machine learning engineering on Databricks Free Edition with Lakeflow Declarative Pipelines, SCD Type 2 history tracking, and MLflow.

[![Databricks](https://img.shields.io/badge/Databricks-Free%20Edition-FF3621?logo=databricks)](https://databricks.com)
[![MLflow](https://img.shields.io/badge/MLflow-Experiment%20Tracking-0194E2?logo=mlflow)](https://mlflow.org)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python)](https://python.org)
[![SCD2](https://img.shields.io/badge/SCD-Type%202-green)](docs/DATA_ARCHITECTURE.md)
[![Great Expectations](https://img.shields.io/badge/Great%20Expectations-Data%20Quality-orange)](https://greatexpectations.io)

---

## Table of Contents

1. [Project Overview](#-project-overview)
2. [ML Portfolio Framework Alignment](#-ml-portfolio-framework-alignment)
3. [Architecture](#-architecture)
4. [Quick Start](#-quick-start)
5. [Component Deep Dives](#-component-deep-dives)
6. [Project Structure](#-project-structure)
7. [Success Metrics](#-success-metrics)
8. [Future Enhancements](#-future-enhancements)

---

## Project Overview

### Problem Statement

Chicago's 311 service handles **millions of non-emergency requests annually** - from pothole repairs to garbage collection issues. City operations teams face a critical challenge: **detecting unusual spikes in service requests before they overwhelm resources**.

### Solution

This platform provides:
- **Demand Forecasting**: Predict future 311 request volumes using Prophet time series models
- **Anomaly Detection**: Identify unusual spikes using model-based thresholds
- **Operational Dashboard**: Real-time insights for resource planning
- **Historical Analysis**: Full lifecycle tracking of requests via SCD Type 2

### Target Users

| User | Need | How This Helps |
|------|------|----------------|
| **City Operations Manager** | Plan staffing for upcoming week | 7-day demand forecasts by ward |
| **311 Call Center Supervisor** | Spot unusual activity | Real-time anomaly alerts |
| **Resource Planner** | Allocate resources by service type | Historical patterns + predictions |
| **Data Analyst** | Understand request lifecycle | Time-in-status analysis via SCD2 |

---

## ML Portfolio Framework Alignment

This project follows the [8-component ML portfolio framework](docs/PORTFOLIO_FRAMEWORK.md) for production-grade projects:

| Component | Status | Implementation |
|-----------|--------|----------------|
| 1. **Problem Framing & Metrics** | âœ… | Business KPIs + ML metrics defined |
| 2. **Unique Data Sourcing** | âœ… | Chicago Data Portal API with continuous collection |
| 3. **Data Storage** | âœ… | Delta Lake with SCD Type 2 (Medallion Architecture) |
| 4. **Feature Engineering** | âœ… | Temporal, categorical, and lag features |
| 5. **Labeling Strategy** | âœ… | Programmatic labeling via Prophet thresholds |
| 6. **Model Training & Evaluation** | âœ… | MLflow experiment tracking + hyperparameter tuning |
| 7. **Deployment** | âœ… | Batch predictions + Streamlit dashboard |
| 8. **Monitoring & Feedback** | âœ… | Prediction logging + Great Expectations data quality |

See [docs/PORTFOLIO_FRAMEWORK.md](docs/PORTFOLIO_FRAMEWORK.md) for detailed alignment documentation.

---

## Architecture

### High-Level Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CHICAGO 311 INTELLIGENCE PLATFORM                      â”‚
â”‚                      (Databricks Free Edition + Lakeflow)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA INGESTION (Continuous Collection)                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
â”‚  â”‚ Chicago Data    â”‚ â”€â”€APIâ”€â”€â–¶ Scheduled Jobs â”€â”€â–¶ Bronze Layer               â”‚
â”‚  â”‚ Portal API      â”‚         (Daily at 6 AM)                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAKEFLOW DECLARATIVE PIPELINE (ETL with SCD Type 2)                        â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚    BRONZE      â”‚     â”‚       SILVER        â”‚     â”‚      GOLD      â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚  â”‚  â”‚ Raw JSON â”‚  â”‚ â”€â”€â–¶ â”‚  â”‚  SCD Type 2   â”‚  â”‚ â”€â”€â–¶ â”‚ â”‚ Aggregates â”‚ â”‚     â”‚
â”‚  â”‚  â”‚ Landing  â”‚  â”‚     â”‚  â”‚  History      â”‚  â”‚     â”‚ â”‚ Features   â”‚ â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â”‚ __START_AT    â”‚  â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”‚ __END_AT      â”‚  â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚  â”‚  â”‚ Staged   â”‚  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚ â”‚ Status     â”‚ â”‚     â”‚
â”‚  â”‚  â”‚ Cleaned  â”‚  â”‚     â”‚         â”‚           â”‚     â”‚ â”‚ Transitionsâ”‚ â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚         â–¼           â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                         â”‚  â”‚Current View   â”‚  â”‚                            â”‚
â”‚  APPLY CHANGES INTO â”€â”€â–¶ â”‚  â”‚(__END_AT=NULL)â”‚  â”‚                            â”‚
â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                            â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                              â”‚
â”‚  Data Quality Expectations enforced at each layer                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML PIPELINE (MLflow Tracked)                                               â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Feature Store   â”‚â”€â”€â”€â–¶â”‚ Prophet Model   â”‚â”€â”€â”€â–¶â”‚ Anomaly Scorer  â”‚          â”‚
â”‚  â”‚ (Gold Tables)   â”‚    â”‚ Training        â”‚    â”‚ (Thresholds)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                â”‚                        â”‚                   â”‚
â”‚                                â–¼                        â–¼                   â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚                         â”‚   MLflow    â”‚         â”‚  Anomalies  â”‚             â”‚
â”‚                         â”‚ Experiments â”‚         â”‚   Table     â”‚             â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SERVING & MONITORING                                                       â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Streamlit     â”‚    â”‚  Prediction     â”‚    â”‚   Data Drift    â”‚          â”‚
â”‚  â”‚   Dashboard     â”‚    â”‚    Logging      â”‚    â”‚   Detection     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SCD Type 2: Why It Matters

311 service requests **change over time** (Open â†’ In Progress â†’ Closed). SCD Type 2 preserves this history:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sr_number â”‚ status       â”‚ __START_AT          â”‚ __END_AT            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SR12345    â”‚ Open         â”‚ 2024-12-01 09:00:00 â”‚ 2024-12-05 14:00:00 â”‚
â”‚ SR12345    â”‚ In Progress  â”‚ 2024-12-05 14:00:00 â”‚ 2024-12-18 10:00:00 â”‚
â”‚ SR12345    â”‚ Closed       â”‚ 2024-12-18 10:00:00 â”‚ NULL                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This enables:
- âœ… **Time-in-Status Analysis**: "How long do noise complaints stay open?"
- âœ… **Point-in-Time Queries**: "What was the status on December 10th?"
- âœ… **Lifecycle Analytics**: "What's the typical path from Open to Closed?"

### Technology Stack

| Layer | Technology | Why This Choice |
|-------|------------|-----------------|
| **Compute** | Databricks Free Edition | Zero cost, serverless, full Lakeflow access |
| **Storage** | Delta Lake + Unity Catalog | ACID transactions, time travel, governance |
| **ETL** | Lakeflow Declarative Pipelines | Declarative SQL/Python, auto-optimization |
| **SCD2** | `APPLY CHANGES INTO` | Built-in CDC handling for history tracking |
| **Data Quality** | Great Expectations | Declarative expectations, data docs, profiling |
| **ML Tracking** | MLflow | Native Databricks integration, experiment management |
| **Forecasting** | Prophet | Handles seasonality, missing data, outliers |
| **Dashboard** | Streamlit | Fast prototyping, Python native |
| **CI/CD** | GitHub Actions | Free tier, Databricks CLI integration |

---

## Quick Start

### Prerequisites

- [Databricks Free Edition Account](https://docs.databricks.com/en/getting-started/free-edition.html)
- Python 3.11+
- Git

### Step 1: Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/nyc311-intelligence-platform.git
cd nyc311-intelligence-platform
```

### Step 2: Set Up Databricks Free Edition

1. Sign up at [Databricks Free Edition](https://www.databricks.com/try-databricks#account)
2. Choose "Express Setup" (no cloud account needed)
3. Create a workspace

### Step 3: Import Notebooks

```bash
# Option A: Import via Databricks UI
# Navigate to Workspace â†’ Import â†’ Upload notebooks folder

# Option B: Use Databricks CLI
pip install databricks-cli
databricks configure --token
databricks workspace import_dir ./notebooks /Workspace/Users/YOUR_EMAIL/nyc311
```

### Step 4: Create Lakeflow Pipeline

1. Navigate to **Workflows** â†’ **Lakeflow Declarative Pipelines**
2. Click **Create Pipeline**
3. Point to `pipelines/nyc311_scd2_pipeline.sql`
4. Configure: **Serverless**, **Development mode**
5. Run the pipeline

### Step 5: Run ML Training

1. Open `notebooks/04_ml_forecasting.py`
2. Attach to serverless compute
3. Run all cells

### Step 6: Launch Dashboard (Local)

```bash
cd app
pip install -r requirements.txt
streamlit run dashboard.py
```

---

## Component Deep Dives

### 1. Problem Framing & Success Metrics

**See:** [docs/PROJECT_SCOPING.md](docs/PROJECT_SCOPING.md)

| Metric Type | Metric | Target | Current |
|-------------|--------|--------|---------|
| **Business** | Anomaly Detection Lead Time | >2 hours before spike | TBD |
| **Business** | False Positive Rate | <15% | TBD |
| **ML** | Forecast MAPE | <15% | TBD |
| **ML** | Anomaly Precision | >0.80 | TBD |

### 2. Data Sourcing

**See:** [docs/DATA_SOURCING.md](docs/DATA_SOURCING.md)

- **Source**: Chicago Open Data Portal (Socrata API)
- **Endpoint**: `https://data.cityofnewyork.us/resource/erm2-nwe9.json`
- **Volume**: ~8,000-12,000 requests/day
- **Collection**: Daily incremental loads

### 3. Data Storage (Medallion + SCD2)

**See:** [docs/DATA_ARCHITECTURE.md](docs/DATA_ARCHITECTURE.md)

| Layer | Table | Description | SCD Type |
|-------|-------|-------------|----------|
| Bronze | `bronze_raw_311_requests` | Raw JSON landing | Append-only |
| Bronze | `bronze_staged_311_requests` | Cleaned, ready for CDC | Append-only |
| Silver | `silver_scd2_311_requests` | Full history | **SCD Type 2** |
| Silver | `silver_current_311_requests` | Current state view | View |
| Silver | `silver_status_history` | Status change history | View |
| Gold | `gold_daily_aggregates` | ML features | Derived |
| Gold | `gold_citywide_daily_summary` | Prophet input | Derived |
| Gold | `gold_status_transitions` | Lifecycle analytics | Derived |

**Key SCD2 Query Patterns:**

```sql
-- Current state only
SELECT * FROM silver_current_311_requests WHERE status = 'OPEN';

-- Full history for a request
SELECT * FROM silver_scd2_311_requests 
WHERE sr_number = 'SR12345' ORDER BY __START_AT;

-- Point-in-time query
SELECT * FROM silver_scd2_311_requests
WHERE sr_number = 'SR12345'
  AND __START_AT <= '2024-12-10'
  AND (__END_AT > '2024-12-10' OR __END_AT IS NULL);

-- Time-in-status analysis
SELECT status, AVG(hours_in_status) as avg_hours
FROM silver_status_history
GROUP BY status;
```

### 4. Feature Engineering

**See:** [docs/FEATURE_ENGINEERING.md](docs/FEATURE_ENGINEERING.md)

**Temporal Features:**
- Day of week, month, quarter
- Is weekend, is holiday
- Days since last spike

**Lag Features:**
- 7-day rolling average
- 28-day rolling average
- Year-over-year comparison

**Categorical Features:**
- Ward encoding
- Complaint type groupings
- Agency assignment

### 5. Labeling Strategy (Anomaly Detection)

**See:** [docs/LABELING_STRATEGY.md](docs/LABELING_STRATEGY.md)

We use **programmatic labeling** via Prophet model thresholds:

```python
# Anomaly = Actual > Upper 95% Confidence Interval
is_anomaly = actual_count > forecast_upper_bound
anomaly_score = (actual_count - forecast_upper_bound) / forecast_upper_bound
```

### 6. Model Training & Evaluation

**See:** [docs/MODEL_DEVELOPMENT.md](docs/MODEL_DEVELOPMENT.md)

- **Algorithm**: Prophet (handles missing data, holidays, multiple seasonalities)
- **Hyperparameter Tuning**: Grid search on validation set
- **Evaluation**: Time-series cross-validation with expanding window
- **Tracking**: All experiments logged to MLflow

### 7. Deployment

**See:** [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md)

| Deployment Type | Description | Update Frequency |
|-----------------|-------------|------------------|
| **Batch Predictions** | Daily forecasts written to Gold table | Every 24 hours |
| **Streamlit Dashboard** | Interactive visualization | Real-time query |

### 8. Monitoring & Feedback

**See:** [docs/MONITORING.md](docs/MONITORING.md)

**Logged for every prediction:**
- Timestamp
- Input features
- Predicted value
- Confidence interval
- Model version

**Alerts:**
- Data quality expectation failures
- Prediction accuracy degradation (MAPE > threshold)
- Missing data in source API

---

## ğŸ“ Project Structure

```
nyc311-intelligence-platform/
â”‚
â”œâ”€â”€ README.md                           # You are here
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”‚
â”œâ”€â”€ docs/                               # Documentation
â”‚   â”œâ”€â”€ PORTFOLIO_FRAMEWORK.md          # How this project maps to ML portfolio framework
â”‚   â”œâ”€â”€ PROJECT_SCOPING.md              # Problem definition & success metrics
â”‚   â”œâ”€â”€ DATA_SOURCING.md                # Data collection strategy
â”‚   â”œâ”€â”€ DATA_ARCHITECTURE.md            # Medallion + SCD2 architecture design
â”‚   â”œâ”€â”€ FEATURE_ENGINEERING.md          # Feature documentation
â”‚   â”œâ”€â”€ LABELING_STRATEGY.md            # How anomalies are labeled
â”‚   â”œâ”€â”€ MODEL_DEVELOPMENT.md            # Model training approach
â”‚   â”œâ”€â”€ DEPLOYMENT.md                   # Deployment strategy
â”‚   â””â”€â”€ MONITORING.md                   # Monitoring & feedback loops
â”‚
â”œâ”€â”€ notebooks/                          # Databricks notebooks
â”‚   â”œâ”€â”€ 00_setup_exploration.py         # Initial setup & EDA
â”‚   â”œâ”€â”€ 01_data_quality_checks.py       # Great Expectations validation
â”‚   â”œâ”€â”€ 02_feature_engineering.py       # Feature development
â”‚   â”œâ”€â”€ 03_model_experimentation.py     # Model experiments
â”‚   â”œâ”€â”€ 04_ml_forecasting.py            # Production model training
â”‚   â”œâ”€â”€ 05_anomaly_detection.py         # Anomaly scoring
â”‚   â””â”€â”€ poc_incremental_scd2.py         # SCD2 proof of concept demo
â”‚
â”œâ”€â”€ pipelines/                          # Lakeflow Declarative Pipelines
â”‚   â”œâ”€â”€ chi311_scd2_pipeline.sql        # Main ETL with SCD Type 2
â”‚   â””â”€â”€ expectations/                   # Legacy expectations (deprecated)
â”‚       â””â”€â”€ data_quality_rules.yaml
â”‚
â”œâ”€â”€ great_expectations/                 # Great Expectations configuration
â”‚   â”œâ”€â”€ great_expectations.yml          # GE configuration
â”‚   â””â”€â”€ expectations/                   # Expectation suites
â”‚       â”œâ”€â”€ bronze_311_requests_suite.json
â”‚       â”œâ”€â”€ silver_311_requests_suite.json
â”‚       â””â”€â”€ gold_daily_aggregates_suite.json
â”‚
â”œâ”€â”€ src/                                # Python source code
â”‚   â””â”€â”€ chi311/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ ingestion/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ api_client.py           # Chicago Open Data API client
â”‚       â”œâ”€â”€ features/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ transformers.py         # Feature engineering functions
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ forecaster.py           # Prophet model wrapper
â”‚       â”œâ”€â”€ monitoring/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ logger.py               # Prediction logging
â”‚       â””â”€â”€ quality/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ ge_validator.py         # Great Expectations utilities
â”‚
â”œâ”€â”€ tests/                              # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ test_api_client.py
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ app/                                # Streamlit dashboard
â”‚   â””â”€â”€ dashboard.py
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â”œâ”€â”€ ci.yml                      # Continuous integration
        â””â”€â”€ deploy.yml                  # Deployment automation
```

---

## Success Metrics

### Business Metrics

| Metric | Description | Target | Measurement |
|--------|-------------|--------|-------------|
| **Detection Lead Time** | Hours before spike is detected | â‰¥2 hours | Compare alert time vs actual spike |
| **False Positive Rate** | % of alerts that weren't real spikes | â‰¤15% | Manual review of alerts |
| **Resource Efficiency** | Staff hours saved via forecasting | 10%+ | Operations team feedback |

### ML Metrics

| Metric | Description | Target | Current |
|--------|-------------|--------|---------|
| **MAPE** | Mean Absolute Percentage Error | â‰¤15% | - |
| **RMSE** | Root Mean Squared Error | â‰¤200 | - |
| **Anomaly Precision** | True positives / All flagged | â‰¥0.80 | - |
| **Anomaly Recall** | True positives / All actual anomalies | â‰¥0.70 | - |

### Data Engineering Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| **SCD2 Version Rate** | Avg versions per request | 1.5-3.0 |
| **Pipeline Latency** | Bronze to Gold processing time | <30 min |
| **Data Freshness** | Hours since latest record | <24 hours |

---

## Future Enhancements

### Phase 2: Advanced Features
- [ ] Real-time streaming with Kafka
- [ ] Multi-step forecasting (7, 14, 30 days)
- [ ] Per-ward specialized models
- [ ] Weather data integration

### Phase 3: Production Hardening
- [ ] A/B testing framework for model versions
- [ ] Automated retraining triggers
- [ ] SLA monitoring and alerting
- [ ] Cost optimization analysis

### Phase 4: Advanced Data Engineering
- [ ] SCD Type 4 (separate current dimension)
- [ ] Data Vault modeling for complex relationships
- [ ] Real-time CDC from operational systems

---

## Learning Resources

Based on the ML portfolio framework, these resources helped build this project:

- ğŸ“• [Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/) - Chip Huyen
- ğŸ“— [Software Engineering for Data Scientists](https://www.oreilly.com/library/view/software-engineering-for/9781098136194/) - Catherine Nelson
- ğŸ“˜ [Databricks Lakeflow Documentation](https://docs.databricks.com/en/dlt/)
- ğŸ“™ [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- ğŸ““ [The Data Warehouse Toolkit](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/books/data-warehouse-dw-toolkit/) - Kimball (SCD patterns)

---

## License

MIT License - see [LICENSE](LICENSE) for details.

---

